NOTE on acronyms:
KLR - Kernel learning rate
MLR - Multi-layer perceptron learning rate
BS - batch size
S= - stride = 

MODEL 1
128x128x3 → 124x124x15 → 120x120x75 → 4x4x75 sum pooling → 1200 → 512 → 47 
            5x5 kernels  5x5 kernels
11 mins for 1000 training examples (256x256 images)
56 seconds for 100 training examples (128x128 single thread)
43 seconds for 100 training examples (128x128 no alpha mutli thread)
7 mins 35 seconds for 1000 training examples (128x128 no alpha mutli thread)
12% accuracy after 43,000 training examples
15% accuracy after 53,000 training examples
21% after 62,000
Still around 21% at 82,000
25% after 117,000
Still has similar accuracy on training data and so overfitting has not occurred
25% after 162,000 (11 epochs)
0.0001f, 0.01f is good
Batch size 10 was used

MODEL 2
242x234x3 -> 118x118x30 -> 58x58x90 -> 28x28x180 -> 4x4x180 max pooling -> 2880 -> 1440 -> 47
        30 7x7 s=2    90 3x3 s=2   180 3x3 s=2       
Batch size of 64
times for 256 training examples (includes the initial save):
images threads, cnn threads, batch size
1,1,64: 1:30 missed 4
1,4,64: 1:12 missed 10
4,4,64: 0:57 missed 6
8,4,64: 0:49 missed 14
4,8,64: 0:55 missed 18
8,8,64: 0:46 missed 14
16,8,64: 0:56 missed 16
16,4,64: 1:10 missed 13
4,4,128 0:45 missed 4
2x as quick as before
16,000 training examples took 1:08:56 and got 5% accuracy with KLR = 10^-4, MLR = 10^-6
16,640 KLR = 5*10^-4, MLR = 10^-5  6%
32,640 8%
45,440 8%
I think the learning rate is too high - only a few popular classes make up the majority of predictions
4,480 KLR = 10^-4, MLR = 10^-6 0%
20,480 2%
Learning rate is not too high
36,480 KLR = 5*10^-4, MLR = 5*10^-6 2%
68,480 9%
I think the model might be the issue

MODEL 1 (AGAIN)
Back to the original model (but with max pooling)
Interesting, the old model seems to have more of a favourite at first
0 3% KLR = 10-2, MLR = 10^-4
new old model (i.e. old with max pooling and mult-threading) takes 1:12 for 256 training examples (similar performance)
16,000 13% (already greater than the previous model)
32,000 12%
learning rate too high?
16,000 KLR = 10^-3, MLR = 10^-5 4%
27,500 KLR = 2*10^-3, MLR = 2*10^-5 8%
43,500 8%
55,660 KLR = 5*10^-3, MLR = 5*10^-5 11%
71,660 15%
87,660 17%
118,100 15%

MODEL 3
new model + similar plants error function
230x230x3 → 112x112x30 → 110x110x60 → 54x54x120 → 26x26x120 → 12x12x240 → 2x2x240 → 960 → 960 → 47 
    30 7x7 s=2      60 5x5    120 3x3 s=2  120 3x3 s=2   240 3x3 s=2   Max Pool
1:03 for 256 images (missed 8) with 2nd kernel as 3x3
1:16 with a 5x5 in second layer
Kernel values are around 10^-1 and kernel grad was 10^-2
Weights are around 10^-2 and weights grad was around 10^-1 
BS=64 5*10^-4,5*10^-6
2,560 5%
16,000 5%
32,000 7%
42,550 10%
80,950 10%

REAL CONVOLUTIONS (MODEL 1)
Changed how convolutions work. Different channels can now interact - see notes
Also now has padding
Takes 1 second for a forwards pass
Takes 3 seconds for a backwards pass
6:41 for 256 images (missed 14)
6x slower
KLR = 10^-4
MLR = 2*10^-5
As much as I want an apples to apples comparison, 6hrs for 1 epoch isn't worth it

MODEL 4 (little model with real convolutions)
128x128x3
64x64x30 (3x3x30 conv stride 2)
32x32x60 (3x3x60 conv stride 2)
16x16x120 (3x3x120 conv stride 2)
4x4x120 (max pool)
1920 FC
1920 FC
47 FC
KLR = 5*10^-4, MLR = 10^-5
1:07 for 256 images (missed 8)
16,000
1 hr(s) 40 min(s) 19 sec(s)
1.0% Semi-decent variety
Learning rate too low or too high?
KLR = 10^-5, MLR = 10^-6
Took: 1 hr(s) 57 min(s) 48 sec(s)
2.0% semi-decent varriety

Debug:
Methods:
convolution - Works without padding, padding had a bug (not all the original values were copied (didn't add ykernelRadius)) - not sure how much of an effect it had
maxPool - Haven't 100% checked the output but, upon small inspection and logical check, seems to work
parseImg - works (proof by compressionTest)
poolingConvBackwards ------\
finalPoolingConvBackwards -- The logic on the convolution bounds for padding were wrong. This would majorly stop learning as for a stride of 2, the kernel could be accidentally be learning on the pixels it didn't affect
convBackwards -------------/
mlpBackwards - Read through checked
backwards - Think this works
forwards - Think this works
reset - Read through checked
CNN - Read through checked

Check LR:
Aiming for each 64 training examples to produce a gradient that could scale up to the magnitude of the current value every 500 batches
LR*maxValue*500 = initValue
LR = initValue/(maxValue*500)
Max kernel gradient was 20 so assume 50
KLR = 2*10^-5 
Biggest weight gradient is around 200 so assume 500
MLR = 4*10^-8, will use 10^-7

0 2% Bias to parlor palm
16,000 5%,3%,4%,0% Varied outputs
KLR = 10^-4, MLR = 5*10^-7 
32,000 4%,3%,4%

No padding 
142x142x3
70x70x30 (3x3x30 conv stride 2)
34x34x60 (3x3x60 conv stride 2)
16x16x120 (3x3x120 conv stride 2)
4x4x120 (max pool)
1920 FC
1920 FC
47 FC
KLR = 10^-4, MLR = 5*10^-7 
0 3%,3%
16,000 4%,4%,3%
KLR = 10^-3,MLR = 10^-5
32,000 2%,3%,5%,2%

Fixed the biases
KLR = 10^-3
MLR = 10^-5
0 3% Prayer Plant Heavy Bias
16,000 1%

try a really small model e.g. 4x4->3x3 and check it does what you think
after doing this, I discovered that the bias has a very large impact
therefore, correcting the (not) summation of the bias derivatives bug may fix the model
Turned off biases and padding
0 3%
14,080 6%,7%,2%,3%,4%,5%,6%,6%,5%
This might actually be real accuracy
I've turned off the biases, padding and similar plants
Keep training - If it gets better, keep going, else lower LRs
30,080 5%,4%,4%,8%,7%,7%,6%
ChatGPT can't find anything and so maybe it is the learning rate
I also compared it to the working code and everything looks pretty similar
LR is too high?
KLR = 2*10^-4
MLR = 5*10^-6
0 1%
16,000 3%,2%

KLR = 10^-3
MLR = 10^-5 
is good
I'll turn back on biases and padding and see if I can get good accuracy
Similarity is still turned off
Also added 5x5 convolutions as these did well originally and make logical sense for plants
128x128x3
64x64x30 (5x5x30 conv stride 2)
32x32x60 (3x3x60 conv stride 2)
16x16x120 (3x3x120 conv stride 2)
4x4x120 (max pool)
1920 FC
960 FC
47 FC

256 6%,3%,0%
16,000 9%,6%,5%,3%,10%,6%,1%,4%,10% mean = 6%
Major Lilium bias (but does always get lilium correct)
32,000 1%,6%,8%,8%,6%,10%,9%,7%,5% mean = 6.7%
Still gets (almost) every Lilium correct but also tries others namely, ZZ plant
48,000 4%,6%,2%,6%,4%,7%,3%,4%,5% mean = 4.6%
KLR = 10^-2,MLR = 10^-4
64,000 6%,5%,6%,5%,6%,3%
Training seems unstable
Methods to fix:
LR too high - current configuration not tested on a lower LR
Too many negatives - a negative output will produce a small gradient due to leakyRelu.
Therefore, negative weights could get stuck. Especially biases as outputs can be close to 0
and so adding a negative bias every time will stop learning
Add softmax and derivative - stops crazy large errors and makes it a bit more properer

MODEL 5
Added:
He Initialisation
Input normalisation
Softmax with cross entropy loss
New image loading library 

256 training examples - 1min 42secs
LR is now for both MLP and Convolutional layers
LR = 10^-4
0: 0%,0%,0%,1%,1%,0%
6,400: 10%,12%,13%,12%,8%,11%,13% mean: 11.3%
12,800: 14%,18%,12%,17%,17%,8%,13%,10% mean: 13.6%
19,200: 19%,15%,8%,11%,8%,14%,11%,13%,15%,17% mean: 13.1%
LR = 10^-5
6,400: 6%,3%,4%,1%,1%,4%,3%,9% mean: 3.9%
19,200: 11%,5%,4%,4%,3%,3%,3%,6% mean: 4.9%
35,200: 7.4% (1000 testing examples)
LR = 4*10^-5
51,200: 10.7%
67,700: 14.4%
83,700: 16.2%
99,700: 16.8%
128,500: 20.8%
156,020: 21.8%
184,180: 21.3% Training accuracy: 36%

MODEL 6 (C++)

08/08/25 5:50s for 256 training examples in gdb 
09/08/25 0:31s,0:37s,0:31s no gdb and with -o3 and 4 cnn threads
0:29s,0:27s,0:29s with 8 cnn threads 
0:33s with 16 cnn threads
Forwards roughly 200ms, taken up by convs 
Backwards roughly 200ms taken up by last conv

DEBUG:
After 16,00 training examples, it only says African violet
But that's because it's the first class and the rest are (mostly) evenly distributed
[0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.000000,0.023256,0.023256,0.023256,0.000000,0.023256,0.023256,0.000000,0.000000,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256,0.023256]
But it's because the outputs are really large and end up clamped
{123.341263, 108.551819, 57.4981422, 129.085175, 74.7348175, 58.4828987, 16.6047897, 15.3049164, 110.077644, 54.0149651, 40.9103813, 46.3508682, 32.5868187, -7.73205614, 55.7830772, 63.4464989, 61.0087013, -38.4879494, 18.8093796, 
  46.4709625, -8.28339005, -78.0414505, 28.6000462, 37.9665184, 127.279472, 44.3927536, 201.433502, 85.7886124, 121.4646, 104.041618, 147.271072, 176.934586, 79.4416656, 154.495377, 131.144302, 145.309769, 212.265213, 281.879211, 
  272.92157, 281.970032, 307.824432, 309.776917, 293.203094, 236.49025, 277.126312, 273.990509, 225.721497}
Biases are also still 0

Resetting
The outputs are pretty normal at first:
{-1.23986936, 1.13382602, 2.78243566, 3.64577222, 3.28940105, 0.197999239, 2.6237421, 3.09681726, -1.57138872, 2.05659819, -0.529989958, 1.66109061, 1.03304327, -1.66569626, -3.21713972, 5.86559916, 1.03555071, 3.03869939, 
  1.08741248, 3.02136302, 6.28720856, 0.811419487, 1.69564271, 6.08814573, -2.10258079, 6.48225164, -0.92306, 3.20509505, -0.451221883, 1.89556098, -1.56058896, 4.23787212, 2.41577458, -4.04050732, 6.24533558, -1.12677574, -1.06042707, 
  0.865875483, 6.82979107, 2.12361407, -1.69991386, 5.75384235, 5.19085979, -0.761214375, -2.14605021, -1.60298073, -1.16697264}

Index error in MLP forwards and finalPoolingConvBackwards


Took: 0 hr(s) 30 min(s) 38 sec(s) for 16,000
Now only says prayer plant?
Gives a very similar output vector for different images
There was an issue with offsets not being added and no biases being present

0 4% Snake plant,Peace lily and Birds Nest Fern with a few other
12,000 said very large gradient and only outputs Anthurium and Areca Palm but this is the weights that were saved before very large gradient
Restarting
0  Says a range of things with the most common being Jade plant
64 Says a range of things with the most common being Poinsettia
128 Says a range of things with no clear mode

Same test on the Java version:
0 Says a small range of things with the most common being Sago Palm
64 Says a wide range of things with no clear mode
128 Says a wide range of things with no clear mode

I think it might have done the v large gradient thing before and so I'll have another punt
16,000 only says Anthurium, African Violet, Begonia, Areca Palm, Boston Fern 
These are the low index classes (A,B) and so I suspect that only the top section of the network is being used 
No not really: 
[0.000000,0.000000,0.076756,0.076756,0.000000,0.000000,0.000000,0.000000,0.076756,0.000000,0.000000,0.000000,0.000000,0.000000,0.002160,0.076756,0.076756,0.000000,0.000000,0.076756,0.000000,0.000000,0.000000,0.076756,0.000000,0.076756,0.000000,0.000000,0.000000,0.000000,0.076756,0.076756,0.076756,0.000012,0.076756,0.000000,0.000000,0.000000,0.076756,0.000000,0.000000,0.000000,0.000000,0.000001,0.000000,0.000000,0.000000]
Values are pretty big however
{-28.5583267, -4.0140624, 22.5354786, 72.0327682, -16.2396221, -8.40388107, -21.2709446, -18.1731663, 19.1541519, -63.8824615, 
  -15.7719955, -42.8161201, -3.16467977, -31.7826385, -3.09618592, 57.9556999, 155.888077, -21.4545269, -8.59567165, 144.289963, 
  -17.3006573, -15.2665777, -13.3939781, 34.1205521, 24.8665485, 63.0063858, -73.5250778, -64.3423691, -62.3776932, -9.31860447, 
  8.20953751, 13.3306046, 2.3166337, 20.2167549, 64.7790909, -3.27565002, -55.9083023, -30.5076256, 17.9811211, -10.7876148, -48.7682457, 
  -27.0873661, -30.4592381, -9.3761816, -31.6270638, -24.1872387, 7.38993025}

Fixed slicing vector tail issue 
30,000 only says African Violet 
Fixed float but should be int issue 
10,240 only says African Violet
Neither ChatGPT or Claude can find any real issues 
Time for small case debugging
3x3x3 -Conv-> 3x2x2 -MP-> 3x1x1 -FC-> 2
Only 1 thread for now
Didn't really find anything up until the backpropagation mess
I then gave ChatGPT a smaller chunk and it found an error (multiplying the index by the wrong layer size)
8,960 only says African Violet

Do a side by side debug with Java
Make a little CNN 
Give it a predefined input and weights
start at high level and then work down to the bug 

3x3x3 -3x3 Conv,s=2-> 2x2x2 -3x3 Conv,s=2-> 2x2x2 -Max Pool-> 2x1x1 -> 2 -FC-> 2
kernelWeights:
[
[
  [
    [[1,2,3],[4,5,6],[7,8,9]],
    [[1,2,3],[4,5,6],[7,8,9]],
    [[1,2,3],[4,5,6],[7,8,9]]
  ],
  [
    [[1,1,1],[1,1,1],[1,1,1]],
    [[1,1,1],[1,1,1],[1,1,1]],
    [[1,1,1],[1,1,1],[1,1,1]]
  ]
],
[
  [
    [[1,2,3],[4,5,6],[7,8,9]],
    [[1,2,3],[4,5,6],[7,8,9]]
  ],
  [
    [[2,2,2],[2,2,2],[2,2,2]],
    [[-1,-1,-1],[-1,-1,-1],[-1,-1,-1]]
  ]
]
]

kernelBiases:
[
  [1,2],
  [3,4]
]

mlpWeights:
[
  [
    [1,2],
    [3,4]
  ]
]

mlpBiases:
[
  [5,1]
]

input:
"African Violet (Saintpaulia ionantha)"
{
  {{120,121,122},{50,51,52},{190,189,188}},
  {{1,2,3},{4,5,6},{7,8,9}},
  {{212,211,210},{42,41,42},{10,20,30}}
}

C++ forwards activations:
[[845.0878295898438,183.76719665527344],[0.5,0.5]]
Java forwards activations:
[[845.0878,183.7672],[0.5,0.5]]

C++ weightsGrad:
{-422.543915, -91.8835983, 422.543915, 91.8835983}
weightsBiasesGrad:
{-0.5, 0.5}
Java weightsGrad:
{-422.543900,-91.883600,-0.500000}
{422.543900,91.883600,0.500000}

C++ kernelsGrad layer 1:
{38.5590515, 38.5590515, 30.8915405, 38.5590515, 38.5590515, 30.8915405, 
  15.2976065, 15.2976065, 19.122467, 11.3597775, 11.3597775, 11.6157112, 11.3597775, 
  11.3597775, 11.6157112, 2.02068043, 2.02068043, 2.97796488, 38.5590515,
  38.5590515, 30.8915405, 38.5590515, 38.5590515, 30.8915405, 15.2976065,
  15.2976065, 19.122467, 11.3597775, 11.3597775, 11.6157112, 11.3597775, 11.3597775, 
  11.6157112, 2.02068043, 2.02068043, 2.97796488}
Java kernelsGrad layer 1:
38.559050,38.559050,30.891540,38.559050,38.559050 .. 
(It's the same and so are the biases) 

Layer 0 gradient values and biases are also the same 
And they update to the same values after the gradients are applied
(Checked every value for weights and biases, kernels 0 and 1 and biases)

With 1 thread and no sharing of weights and 10,880 training examples,
it predicts multiple classes and gets 9.1%

With 8 threads and no sharing of weights and 10,880 training exmaples,
it predicts multiple classes and gets 2.8%
Activations are large
{9.52171516, -40.1567078, 8.28015232, 8.66102123, -0.458888441, 9.93610191, 6.89729071, 7.46941233, 9.24370861, 8.7864027, 7.94878387, 8.84818745, 8.32969475, 8.16819096, 8.21183395, 7.89964771, 8.80831528, 
  8.28320408, 8.89385128, -12.3722534, -47.7813797, 6.64466476, 7.82463789, 9.29623318, 4.90948296, 7.9115634, 7.03853559, -128.329269, 9.31005573, -47.791748, 8.04968643, 7.9564867, 8.64347553, 7.19745731, 6.98667908, 
  5.76776743, 7.81631613, -26.0173988, 8.28213024, 4.44857264, 9.32877159, 8.34967422, 8.8225832, 7.18084526, 7.32378483, -3.26597977, 8.58827591}
Multiple threads is breaking it

With atomic plant images it still occurs
Added atomic missedCount and stopped doing srand and it still occurs 
Still occurs with 2 threads:
{16.4198666, 16.0234489, 18.0993862, 15.4736547, 16.4040108, 19.3528118, 16.012207, 18.0564442, 18.1161251, 19.6746254, 16.7667599, 21.5374699, 17.0509605, 18.4469357, 14.5579348, 17.9768906, 19.0169907, 17.0598316, 
  20.2746944, 17.7493515, 18.879427, 18.8458996, 16.6811409, 18.2030811, 17.7739658, 16.5759048, 19.0271225, 21.5033913, 19.3551064, 17.4600811, 16.2856541, 15.8312798, 14.7728605, 17.0919132, 19.5311699, 17.9462147,       
  19.7439289, 18.9701748, 19.3921661, 16.5553246, 16.157692, 18.2624874, 17.9802895, 17.5588951, 17.8536415, -815.386414, 17.3839951}

(In debug mode (no optimisation))
When checking for large values in activations,maps,kernels,weights,biases,grads,
the first error that occurs is this:
Large weight grad value of 102.503 at layer 1 flat index 12070
Large weight grad value of 103.491 at layer 1 flat index 12254
Large weight grad value of 104.828 at layer 1 flat index 12257

and then in later iterations it compounds to this:
Large weight grad value of 140.747 at layer 1 flat index 11527
Large weight grad value of 108.489 at layer 1 flat index 11562
Large weight grad value of 123.733 at layer 1 flat index 11616
Large weight grad value of 107.218 at layer 1 flat index 11692
Large weight grad value of 104.258 at layer 1 flat index 11768
Large weight grad value of 106.328 at layer 1 flat index 11833
Large weight grad value of 112.689 at layer 1 flat index 11862
Large weight grad value of 123.077 at layer 1 flat index 11903
Large weight grad value of 124.428 at layer 1 flat index 11915
Large weight grad value of 124.528 at layer 1 flat index 11964
Large weight grad value of 111.993 at layer 1 flat index 11992
Large weight grad value of 140.662 at layer 1 flat index 12011
Large weight grad value of 115.001 at layer 1 flat index 12059
Large weight grad value of 148.201 at layer 1 flat index 12065
Large weight grad value of 161.449 at layer 1 flat index 12070
Large weight grad value of 135.727 at layer 1 flat index 12090
Large weight grad value of 121.83 at layer 1 flat index 12132

2nd attempt:
Thread 6 hit Breakpoint 1, CNN::forwards[abi:cxx11](Tensor&) (this=0x672b80, imageInt=...) at C:\Users\Alistair\VS-Code-Projects\Plant-CNN-CPP\src\cnn.cpp:256
256                     std::cout << "Large weight grad value of " << gradData[i] << " at layer " << l << " flat index " << i << std::endl;
(gdb) p l
$1 = 1
(gdb) p i
$2 = 44907
(gdb) p weightsGrad[l]
$3 = (__gnu_cxx::__alloc_traits<std::allocator<Tensor>, Tensor>::value_type &) @0x677800: {data = std::shared_ptr<float []> (use count 1, weak count 0) = {get() = 0x6a9ec0}, 
  biases = std::shared_ptr<Tensor> (use count 1, weak count 0) = {get() = 0x677bb0}, offset = 0, dimens = std::vector of length 2, capacity 2 = {47, 960}, childSizes = std::vector of length 2, capacity 2 = {960, 1},        
  totalSize = 45120}
(gdb) p *(float[50]*)(weightsGrad[l].getData()+44900)
$4 = {1.79109991, 1.16593754, 2.89700222, 27.4244404, 0.253519922, 19.0143585, -0.320863664, 105.611282, 11.8430424, 1.35429156, 24.4544697, 6.96354818, 3.31078744, 37.915081, -0.50585407, -0.601735294, -0.396453917, 
  39.266922, 26.5787811, -0.242265865, 7.38034391, 42.5989914, -0.381493092, 18.3094635, 0.0327438898, 19.0387478, 12.768137, 27.3200169, 1.03149199, -0.288701802, 11.7764912, 7.63996172, 11.1631908, 40.530159, 5.0627284, 
  -0.276052892, -0.171619758, 5.75633812, 13.3641748, 9.59098339, 5.27795267, -0.532275856, 2.34461689, 69.4735641, 61.9673882, -0.31826359, 31.3097382, 5.21517086, 2.51162791, 18.0534019}

3rd attempt:
didn't happen

Multi-threaded non-debug version (optimised):
1st (2 batches): didn't happen
2nd (2 batches): only 1 weights grad 103.852 at layer 1 39620
3rd (10 batches): changed to 10 iterations and started to get lots of "Large activation in final layer (>15) at index 17" on 6th and 7th iterations. Aborted
4th (10): only got one "Large activation in final layer (>15) at index 1" on 9th iteration
5th (10): "Large activation in final layer (>15) at index 37" on 3rd iteration and lots of "Large weight grad value of 100.022 at layer 1 flat index 35840" on 0th onwards

Single-threaded non-debug (optimised):
1st (10): "Large kernel grad value of 16.3831 at layer 1 flat index 4724" on 0th,2nd,3rd,5th. Problem went away.
2nd (10): Crazy amounts of large weight and kernel grads on 0th,1st,2nd iterations "Large weight grad value of 234.709 at layer 1 flat index 29753" "Large kernel grad value of 15.2281 at layer 2 flat index 61024". Problem went away.
3rd (10): Crazy amounts of large weight and kernel grads on 0th,1st,2nd iterations "Large kernel grad value of 15.2643 at layer 2 flat index 59296" "Large weight grad value of 113.96 at layer 1 flat index 32988". Problem went away.

Onto debugging side-by-side Java and C++ over 1 batch seeing if they accumulate the same weights at the end given same inputs
1 thread 64 images:
All the weights,kernels and biases were the same except 2
This was because the C++ max gradients is lower than the Java version
After changing this, they are all the same 

2 threaded 64 images:
All the weights,kernels and biases were the same
next iteration:
Still the same 


2 threaded 64 images in -o3 (non-debug):
All the weights,kernels and biases were the same

2 threaded 10 batches back to back in -o3 (non-debug):
All the weights,kernels and biases are differenet 

2 threaded 2 batches back to back in debug mode:
All different 
Initial weights:
[[[1,2],[3,4]]]
C++ weights:
[[[2.955050230026245,2.4332408905029297],[1.0449497699737549,3.5667591094970703]]]
Java weights: 
[[[2.746676,2.396038],[1.253324,3.603962]]]

1 threaded 2 batches back to back in debug mode:
The same

ChatGPT and Claude gave complete waffle
Gemini Pro, however, told me that the vector push_back was calling the copy constructor of Tensor 
and so my supposed weight shallow copies were actually deep copies and so only the original CNNs weights were updating
I then asked the same question (same words and files) to Gemini Pro (I had my history turned off) and it told me something different

2 threaded 2 batches back to back in debug mode:
It works
32:08 for 16,000
16,000: 12.6% 
48,000: 21.4% train dataset: 21.7%

Speeding up:
Times are recorded from -o3
Forwards = 730ms
 - Convoltional layers = 690ms
 - Single 3x3 convolution = 2ms (120+60+30)*2 = 420ms
  Big ones don't seem to take any longer
Backwards = 250ms 
 - finalPoolingConvBackwards = 110ms
 - convBackwards = 65ms

After "optimisation", forwards takes 970ms
Current time for 256 images: 0:23s,0:22s,0:21s
Previous time: 0:29s